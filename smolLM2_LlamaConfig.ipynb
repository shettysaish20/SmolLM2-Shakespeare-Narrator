{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA V3 Session 13 Assignment - SmolLM2-135 model\n",
    "\n",
    "\n",
    "1. Find out the details about SmolLM2-135 model. You won't get model.py free of cost online. You need to read its github, get yaml file for training, and download the 135 model, and then reverse engineer it. \n",
    "2. Training it for 5000 steps while predicting every 500 steps on what it utters. Now fully stop the model and save a checkpoint. Now load this checkpoint and train for 50 more steps.\n",
    "3. Use all the speedups that we have used. \n",
    "4. Submit the GitHub link where I can see your README.md explaining the model definition. Parameter Calculation. Upload the model to Spaces as well and then share both the links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model using LlamaConfig class\n",
    "\n",
    "SmolLM2-135M model has ```is_llama_config = True``` as one of the parameters, which means it can be created using the LlamaConfig Class by inputing the same parameters as written in ```config_smollm2_135M.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoLLM2 model and tokenizer successfully created!\n",
      "Model dtype: torch.bfloat16\n",
      "Tokenizer: GPT2TokenizerFast loaded from HuggingFaceTB/cosmo2-tokenizer\n"
     ]
    }
   ],
   "source": [
    "# model.py\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def create_smollm2_model():\n",
    "    \"\"\"\n",
    "    Constructs a SmoLLM2 model based on the provided configuration.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the initialized model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model_config = LlamaConfig(\n",
    "        vocab_size=49152,\n",
    "        hidden_size=576,\n",
    "        intermediate_size=1536,\n",
    "        num_hidden_layers=30,\n",
    "        num_attention_heads=9,\n",
    "        num_key_value_heads=3,\n",
    "        hidden_act=\"silu\",\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.041666666666666664,\n",
    "        rms_norm_eps=1.0e-05,\n",
    "        # use_cache=True, As seen in training, this is not needed\n",
    "        tie_word_embeddings=True,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        rope_interleaved=False,\n",
    "        pretraining_tp=1,\n",
    "        bos_token_id=0,\n",
    "        eos_token_id=0,\n",
    "        pad_token_id=None, #  pad_token_id is null in config, setting to None\n",
    "    )\n",
    "\n",
    "    model = LlamaForCausalLM(model_config)\n",
    "\n",
    "    # Initialize weights with std from init_method if needed (Transformers usually handles initialization well)\n",
    "    # init_std = 0.041666666666666664\n",
    "    # You can add custom weight initialization here if required based on init_method.std\n",
    "\n",
    "    # Set the dtype to bfloat16\n",
    "    model.to(torch.bfloat16)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer_name_or_path = \"HuggingFaceTB/cosmo2-tokenizer\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = create_smollm2_model()\n",
    "print(\"SmoLLM2 model and tokenizer successfully created!\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer: {tokenizer.__class__.__name__} loaded from {tokenizer.name_or_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='HuggingFaceTB/cosmo2-tokenizer', vocab_size=49152, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<repo_name>', '<reponame>', '<file_sep>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 576,\n",
       "  \"initializer_range\": 0.041666666666666664,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 9,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"num_key_value_heads\": 3,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_interleaved\": false,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.44.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 49152\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 134515008\n",
      "Trainable parameters: 134515008\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params}')\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample output generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: Hello, world!immer Joseph params transactionDateTimeField plagiarism sniffShdis borders savage debit interfering polyunsaturatedothySh\n"
     ]
    }
   ],
   "source": [
    "# Example usage (optional):\n",
    "input_text = \"Hello, world!\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input text and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\"datasets/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing dataset...\n",
      "Dataset prepared with 41 micro-batches using micro_batch_size=4 and sequence_length=2048.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5000 [00:00<?, ?step/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Training:  10%|▉         | 499/5000 [09:56<1:28:35,  1.18s/step, Avg Loss=1.9720]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 500 (Global Step: 125) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be, dm emotionally Whole theware\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "::\n",
      ":::::\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 500/5000 [09:59<2:22:02,  1.89s/step, Avg Loss=1.9720]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 500 (Global Step 125) to smollm2-checkpoints\\smollm2_checkpoint_step_500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 999/5000 [20:54<1:21:06,  1.22s/step, Avg Loss=1.6093]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 1000 (Global Step: 250) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be, of\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1000/5000 [20:56<2:10:11,  1.95s/step, Avg Loss=1.6093]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 1000 (Global Step 250) to smollm2-checkpoints\\smollm2_checkpoint_step_1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 1499/5000 [32:12<1:24:19,  1.45s/step, Avg Loss=1.3746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 1500 (Global Step: 375) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,\n",
      "K\n",
      "KARI: I,\n",
      ", the of of of\n",
      ",,,,,,\n",
      " I\n",
      " IARD\n",
      " I:,,,,,, I\n",
      " I to to\n",
      " I I\n",
      " I:,,,,'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 1500/5000 [32:16<2:08:24,  2.20s/step, Avg Loss=1.3746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 1500 (Global Step 375) to smollm2-checkpoints\\smollm2_checkpoint_step_1500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 1999/5000 [43:52<1:15:02,  1.50s/step, Avg Loss=1.2741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 2000 (Global Step: 500) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,\n",
      " will not:,,,,, so\n",
      ",,,, a\n",
      " I,,,,,,,,, a,\n",
      " I, the the the the the the,\n",
      " I, the the the,\n",
      "\n",
      " you'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2000/5000 [43:55<1:55:21,  2.31s/step, Avg Loss=1.2741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 2000 (Global Step 500) to smollm2-checkpoints\\smollm2_checkpoint_step_2000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 2499/5000 [54:53<50:01,  1.20s/step, Avg Loss=0.9884]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 2500 (Global Step: 625) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be, a to to I\n",
      " a's to to a to\n",
      " a?\n",
      " a to to to\n",
      ", I I I I I\n",
      " a to, that\n",
      " a of, I I I I\n",
      " a of I I I I I\n",
      " a.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 2500/5000 [54:56<1:19:07,  1.90s/step, Avg Loss=0.9884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 2500 (Global Step 625) to smollm2-checkpoints\\smollm2_checkpoint_step_2500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 2999/5000 [1:05:00<40:08,  1.20s/step, Avg Loss=0.6190]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 3000 (Global Step: 750) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be, I, a; the!\n",
      " so is with again my the?\n",
      "N now'd! will the so, it will, I\n",
      " we of or.\n",
      "INGICH II\n",
      " a: I you I so\n",
      " so a- and and.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3000/5000 [1:05:03<1:04:23,  1.93s/step, Avg Loss=0.6190]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 3000 (Global Step 750) to smollm2-checkpoints\\smollm2_checkpoint_step_3000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 3499/5000 [1:15:04<30:02,  1.20s/step, Avg Loss=0.3180]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 3500 (Global Step: 875) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,, a\n",
      " so;! name. but on!\n",
      "all him\n",
      "antIL: I will it\n",
      "s,, of\n",
      " made to,--, or! a\n",
      " by but the made but made and\n",
      " him'.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 3500/5000 [1:15:07<48:46,  1.95s/step, Avg Loss=0.3180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 3500 (Global Step 875) to smollm2-checkpoints\\smollm2_checkpoint_step_3500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 3999/5000 [1:25:09<19:58,  1.20s/step, Avg Loss=0.2034]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 4000 (Global Step: 1000) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, I;, aw,\n",
      "'s thy now;\n",
      " mine butI'\n",
      "P I to it or not by: he king.\n",
      "L will, now I not both not.\n",
      "L those!\n",
      " will at a'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4000/5000 [1:25:12<33:30,  2.01s/step, Avg Loss=0.2034]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 4000 (Global Step 1000) to smollm2-checkpoints\\smollm2_checkpoint_step_4000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 4499/5000 [1:35:13<09:59,  1.20s/step, Avg Loss=0.1876]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 4500 (Global Step: 1125) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, I;, aw,\n",
      "'s thy now;\n",
      " mine butI'\n",
      "P I to it or not by: he king.\n",
      "L will, now I not both not.\n",
      "LENT it.\n",
      " me--'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 4500/5000 [1:35:16<15:29,  1.86s/step, Avg Loss=0.1876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 4500 (Global Step 1125) to smollm2-checkpoints\\smollm2_checkpoint_step_4500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 4999/5000 [1:45:42<00:01,  1.34s/step, Avg Loss=0.1753]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 5000 (Global Step: 1250) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, I;, aw,\n",
      "'s thy now!\n",
      " it\n",
      " didy\n",
      " but! thee\n",
      " sweet,; did,,,, my to,\n",
      " did else and's; the if you\n",
      " will. all do'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [1:45:45<00:00,  1.27s/step, Avg Loss=0.1753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 5000 (Global Step 1250) to smollm2-checkpoints\\smollm2_checkpoint_step_5000.pth\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, AutoModelForCausalLM # Changed import for checkpointing\n",
    "# from model import create_smollm2_model\n",
    "import os\n",
    "\n",
    "# --- 1. Hyperparameters and Configuration ---\n",
    "TRAIN_STEPS = 5000\n",
    "PREDICT_EVERY_STEPS = 500\n",
    "CHECKPOINT_EVERY_STEPS = 500\n",
    "SEQUENCE_LENGTH = 2048  # As defined in config - May need to reduce if OOM\n",
    "MICRO_BATCH_SIZE = 8     # As defined in config - REDUCE THIS FIRST to fix OOM\n",
    "LEARNING_RATE = 3e-4     # You can adjust, simplified from config for now\n",
    "WARMUP_STEPS = 500      # Simplified warmup\n",
    "CHECKPOINT_PATH = \"smollm2-checkpoints\"\n",
    "INPUT_FILE = \"datasets\\input.txt\"\n",
    "PREDICTION_PROMPT = \"To be or not to be,\" # A starting prompt for prediction\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # --- Speedup 1: Gradient Accumulation --- Accumulate gradients over this many steps\n",
    "USE_ACTIVATION_CHECKPOINTING = True # --- Speedup 2: Activation Checkpointing --- Enable or disable activation checkpointing\n",
    "\n",
    "# --- ***MEMORY OPTIMIZATION - REDUCE THESE IF OOM ERROR PERSISTS*** ---\n",
    "REDUCE_MICRO_BATCH_SIZE_FACTOR = 2 # --- Reduce Micro Batch Size --- Reduce micro_batch_size by this factor\n",
    "# If you STILL get OOM, try reducing SEQUENCE_LENGTH_FACTOR (but micro_batch_size reduction is usually more effective first)\n",
    "REDUCE_SEQUENCE_LENGTH_FACTOR = 1 # --- Reduce Sequence Length --- Reduce sequence length by this factor if needed\n",
    "\n",
    "# --- 2. Adjusted Hyperparameters based on Reduction Factors ---\n",
    "ADJUSTED_MICRO_BATCH_SIZE = MICRO_BATCH_SIZE // REDUCE_MICRO_BATCH_SIZE_FACTOR\n",
    "if ADJUSTED_MICRO_BATCH_SIZE <= 0:\n",
    "    ADJUSTED_MICRO_BATCH_SIZE = 1 # Ensure micro_batch_size is at least 1\n",
    "ADJUSTED_SEQUENCE_LENGTH = SEQUENCE_LENGTH // REDUCE_SEQUENCE_LENGTH_FACTOR\n",
    "if ADJUSTED_SEQUENCE_LENGTH <= 0:\n",
    "    ADJUSTED_SEQUENCE_LENGTH = 64 # Ensure sequence_length is at least reasonably sized\n",
    "\n",
    "MICRO_BATCH_SIZE = ADJUSTED_MICRO_BATCH_SIZE # Update micro batch size\n",
    "SEQUENCE_LENGTH = ADJUSTED_SEQUENCE_LENGTH # Update sequence length\n",
    "\n",
    "\n",
    "# --- 3. Create Checkpoint Directory ---\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# --- 4. Load Model and Tokenizer ---\n",
    "model, tokenizer = create_smollm2_model()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Speedup 3: Mixed Precision Training (bfloat16) --- Model is already in bfloat16 from model.py\n",
    "# We are using bfloat16 dtype for model which speeds up training and reduces memory usage on compatible GPUs (like NVIDIA A100, H100)\n",
    "\n",
    "if USE_ACTIVATION_CHECKPOINTING: # --- Enable activation checkpointing if flag is set ---\n",
    "    model.gradient_checkpointing_enable() # Enable activation checkpointing from transformers\n",
    "\n",
    "# --- 5. Prepare Dataset ---\n",
    "print(\"Loading and tokenizing dataset...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenized_dataset = tokenizer(text, return_tensors=\"pt\", truncation=False) # No truncation initially\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "\n",
    "# --- 6. Create Data Batches ---\n",
    "def create_batches(input_ids, seq_len, micro_batch_size):\n",
    "    num_tokens = input_ids.shape[1]\n",
    "    num_batches = num_tokens // seq_len\n",
    "    truncated_input_ids = input_ids[:, :num_batches * seq_len] # Truncate to fit full sequences based on potentially reduced SEQUENCE_LENGTH\n",
    "    batched_input_ids = truncated_input_ids.reshape(-1, seq_len) # Reshape into sequences\n",
    "    num_micro_batches = num_batches // micro_batch_size\n",
    "    micro_batches = []\n",
    "    for i in range(num_micro_batches):\n",
    "        start_index = i * micro_batch_size\n",
    "        end_index = (i + 1) * micro_batch_size\n",
    "        batch = batched_input_ids[start_index:end_index]\n",
    "        micro_batches.append(batch)\n",
    "    return micro_batches\n",
    "\n",
    "micro_batches = create_batches(input_ids.to(device), SEQUENCE_LENGTH, MICRO_BATCH_SIZE) # Using potentially reduced SEQUENCE_LENGTH and MICRO_BATCH_SIZE\n",
    "print(f\"Dataset prepared with {len(micro_batches)} micro-batches using micro_batch_size={MICRO_BATCH_SIZE} and sequence_length={SEQUENCE_LENGTH}.\")\n",
    "\n",
    "\n",
    "# --- 7. Optimizer and Scheduler ---\n",
    "# --- Speedup 4: Fused AdamW Optimizer --- Using fused AdamW if available (often faster)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01, fused=True) # Using fused=True for potential speedup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=TRAIN_STEPS // GRADIENT_ACCUMULATION_STEPS # Adjusted lr scheduler steps\n",
    ") # Simplified scheduler\n",
    "\n",
    "# --- imports ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- training loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train() # Set model to training mode\n",
    "global_step = 0\n",
    "accumulated_loss = 0 # To track loss over accumulation steps\n",
    "\n",
    "# Initialize tqdm progress bar, total is global steps (weight updates)\n",
    "progress_bar = tqdm(range(1, TRAIN_STEPS + 1), desc=\"Training\", unit=\"step\")\n",
    "\n",
    "for step in progress_bar: # Wrap training loop with tqdm\n",
    "    batch_index = (step - 1) % len(micro_batches) # Cycle through batches\n",
    "    batch = micro_batches[batch_index]\n",
    "\n",
    "    inputs = batch\n",
    "    targets = torch.roll(batch, shifts=-1, dims=1) # Next token prediction\n",
    "\n",
    "    outputs = model(inputs, labels=targets) # Labels for loss calculation\n",
    "    loss = outputs.loss\n",
    "    loss = loss / GRADIENT_ACCUMULATION_STEPS # --- Scale loss for gradient accumulation ---\n",
    "    accumulated_loss += loss.item() # Accumulate loss for logging\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if step % GRADIENT_ACCUMULATION_STEPS == 0: # --- Update weights every GRADIENT_ACCUMULATION_STEPS ---\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # Increment global step only when weights are updated\n",
    "        avg_loss = accumulated_loss / GRADIENT_ACCUMULATION_STEPS # Calculate average loss over accumulation steps\n",
    "        accumulated_loss = 0 # Reset accumulated loss\n",
    "\n",
    "        if global_step % 10 == 0: # Log loss every 10 global steps (after accumulation)\n",
    "            # No need for separate print here, tqdm will handle logging\n",
    "\n",
    "            # --- Update tqdm progress bar with current average loss ---\n",
    "            progress_bar.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "\n",
    "    # --- 8. Prediction Interval ---\n",
    "    if step % PREDICT_EVERY_STEPS == 0:\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"\\n--- Prediction at Micro-batch Step {step} (Global Step: {global_step}) ---\") # Clarify step counts\n",
    "        prompt_ids = tokenizer.encode(PREDICTION_PROMPT, return_tensors=\"pt\").to(device)\n",
    "        sample_outputs = model.generate(\n",
    "            prompt_ids,\n",
    "            max_length=len(prompt_ids[0]) + 50, # Generate up to 50 new tokens\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7, # Adjust temperature for creativity\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predicted_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Prompt: '{PREDICTION_PROMPT}'\")\n",
    "        print(f\"Generated: '{predicted_text}'\\n\")\n",
    "        model.train() # Set back to training mode\n",
    "\n",
    "    # --- 9. Checkpoint Saving ---\n",
    "    if step % CHECKPOINT_EVERY_STEPS == 0:\n",
    "        checkpoint_file = os.path.join(CHECKPOINT_PATH, f\"smollm2_checkpoint_step_{step}.pth\") # Step here is still micro-batch step\n",
    "        torch.save({\n",
    "            'step': step, # Step here is still micro-batch step\n",
    "            'global_step': global_step, # Saving global step as well (steps with weight updates)\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss.item(), # Save current micro-batch loss (last loss in accumulation)\n",
    "        }, checkpoint_file)\n",
    "        print(f\"Checkpoint saved at Micro-batch Step {step} (Global Step {global_step}) to {checkpoint_file}\") # Clarified step counts in checkpoint message\n",
    "\n",
    "progress_bar.close() # Close progress bar when training finishes\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained it for 5000 micro-steps (1250 weight-updating steps because of Gradient Accumulation)\n",
    "\n",
    "## Loading Saved Checkpoint and re-training\n",
    "\n",
    "To achieve the intended objective, we will load the checkpoint and further train it till 5000th weight-updating step (20000 total micro-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing dataset...\n",
      "Loading checkpoint from: smollm2-checkpoints\\smollm2_checkpoint_step_5000.pth\n",
      "Resuming training from global step: 1250\n",
      "Dataset prepared with 41 micro-batches using micro_batch_size=4 and sequence_length=2048.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 5000/20000 [00:00<?, ?step/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Training:  27%|██▋       | 5499/20000 [08:44<4:14:06,  1.05s/step, Avg Loss=0.0933]c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 5500 (Global Step: 1375) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, I:\n",
      " most low o, a wife\n",
      " mine thenoth up to lives\n",
      " your power your, itt\n",
      "y of.\n",
      "BDEL from\n",
      "-?\n",
      "N: who it not pray,; not'\n",
      "'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 5500/20000 [08:48<7:41:58,  1.91s/step, Avg Loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 5500 (Global Step 1375) to smollm2-checkpoints\\smollm2_checkpoint_step_5500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 5999/20000 [17:35<4:12:05,  1.08s/step, Avg Loss=0.1063]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 6000 (Global Step: 1500) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be, now with\n",
      " o God make with, his.\n",
      " true; the with world with de, know not\n",
      " would withs I for with son\n",
      " King up ifsRE.\n",
      "B thy to: God; the for with is?\n",
      "A'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 6000/20000 [17:38<6:56:24,  1.78s/step, Avg Loss=0.1063]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 6000 (Global Step 1500) to smollm2-checkpoints\\smollm2_checkpoint_step_6000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 6499/20000 [26:34<4:02:56,  1.08s/step, Avg Loss=0.0257]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 6500 (Global Step: 1625) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      "y'd general a child\n",
      " what weio; my!\n",
      " use play him these you\n",
      " I! will more!\n",
      "S brother' with: by good,\n",
      "end go,\n",
      " father upons'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▎      | 6500/20000 [26:38<6:33:22,  1.75s/step, Avg Loss=0.0257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 6500 (Global Step 1625) to smollm2-checkpoints\\smollm2_checkpoint_step_6500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 6999/20000 [35:38<3:53:10,  1.08s/step, Avg Loss=0.0220]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 7000 (Global Step: 1750) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      "y'd general a child\n",
      " what weio; my!\n",
      " use play him these you\n",
      " I tell thy.\n",
      "N: now,?; else but there:\n",
      " mostw are.\n",
      "First'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 7000/20000 [35:40<6:12:01,  1.72s/step, Avg Loss=0.0220]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 7000 (Global Step 1750) to smollm2-checkpoints\\smollm2_checkpoint_step_7000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 7499/20000 [44:39<3:45:15,  1.08s/step, Avg Loss=0.0195]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 7500 (Global Step: 1875) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      "y'd general a child\n",
      " by upon; it fromv\n",
      " name with before the; thens you\n",
      "umberland be who bute his.\n",
      "CAMLOest can!\n",
      " what the is's,'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 7500/20000 [44:42<5:57:29,  1.72s/step, Avg Loss=0.0195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 7500 (Global Step 1875) to smollm2-checkpoints\\smollm2_checkpoint_step_7500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 7999/20000 [53:42<3:35:37,  1.08s/step, Avg Loss=0.0196]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 8000 (Global Step: 2000) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      "y'd general a child\n",
      " what weio; my!\n",
      " use play him these you\n",
      " I tell thy.\n",
      "N: now,?; else but there:\n",
      " mostw are.\n",
      "First'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 8000/20000 [53:44<5:43:33,  1.72s/step, Avg Loss=0.0196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 8000 (Global Step 2000) to smollm2-checkpoints\\smollm2_checkpoint_step_8000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 8499/20000 [1:02:43<3:26:52,  1.08s/step, Avg Loss=0.0174]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 8500 (Global Step: 2125) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " time no, the will at, I so'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▎     | 8500/20000 [1:02:46<5:30:28,  1.72s/step, Avg Loss=0.0174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 8500 (Global Step 2125) to smollm2-checkpoints\\smollm2_checkpoint_step_8500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 8999/20000 [1:11:47<3:20:13,  1.09s/step, Avg Loss=0.0174]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 9000 (Global Step: 2250) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will,, a and's,\n",
      "--itor'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 9000/20000 [1:11:49<5:31:20,  1.81s/step, Avg Loss=0.0174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 9000 (Global Step 2250) to smollm2-checkpoints\\smollm2_checkpoint_step_9000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 9499/20000 [1:20:58<3:12:58,  1.10s/step, Avg Loss=0.0169]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 9500 (Global Step: 2375) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " time no, the will at, I so'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 9500/20000 [1:21:01<5:02:37,  1.73s/step, Avg Loss=0.0169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 9500 (Global Step 2375) to smollm2-checkpoints\\smollm2_checkpoint_step_9500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 9999/20000 [1:30:12<3:03:26,  1.10s/step, Avg Loss=0.0164]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 10000 (Global Step: 2500) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with,H will,?\n",
      "BK for'd\n",
      " but'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 10000/20000 [1:30:15<4:54:18,  1.77s/step, Avg Loss=0.0164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 10000 (Global Step 2500) to smollm2-checkpoints\\smollm2_checkpoint_step_10000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 10499/20000 [1:39:21<2:52:47,  1.09s/step, Avg Loss=0.0165]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 10500 (Global Step: 2625) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will, a--, a that\n",
      " time to to'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▎    | 10500/20000 [1:39:24<4:40:51,  1.77s/step, Avg Loss=0.0165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 10500 (Global Step 2625) to smollm2-checkpoints\\smollm2_checkpoint_step_10500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 10999/20000 [1:48:32<2:43:55,  1.09s/step, Avg Loss=0.0170]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 11000 (Global Step: 2750) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will, a--, a that--;\n",
      " o'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 11000/20000 [1:48:34<4:20:58,  1.74s/step, Avg Loss=0.0170]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 11000 (Global Step 2750) to smollm2-checkpoints\\smollm2_checkpoint_step_11000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 11499/20000 [1:57:44<2:36:39,  1.11s/step, Avg Loss=0.0163]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 11500 (Global Step: 2875) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, the,, a 's with,\n",
      " by-'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▊    | 11500/20000 [1:57:47<4:10:02,  1.77s/step, Avg Loss=0.0163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 11500 (Global Step 2875) to smollm2-checkpoints\\smollm2_checkpoint_step_11500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 11999/20000 [2:06:58<2:26:19,  1.10s/step, Avg Loss=0.0171]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 12000 (Global Step: 3000) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,, Iy,y, land\n",
      " mine; beingest;est;ate' to,\n",
      " I with up to mad Mar theal\n",
      " did I yourom whoanish and,'d\n",
      " one on no time\n",
      "L now\n",
      " most'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 12000/20000 [2:07:00<3:55:12,  1.76s/step, Avg Loss=0.0171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 12000 (Global Step 3000) to smollm2-checkpoints\\smollm2_checkpoint_step_12000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 12499/20000 [2:16:09<2:17:11,  1.10s/step, Avg Loss=0.0174]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 12500 (Global Step: 3125) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iy, master't; that\n",
      "y it friends: it!\n",
      "sty who but king: then to!\n",
      "th he king will straight there there at man\n",
      " loss'd I a\n",
      " by! that you.\n",
      "'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▎   | 12500/20000 [2:16:12<3:39:45,  1.76s/step, Avg Loss=0.0174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 12500 (Global Step 3125) to smollm2-checkpoints\\smollm2_checkpoint_step_12500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 12999/20000 [2:25:20<2:07:21,  1.09s/step, Avg Loss=0.0184]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 13000 (Global Step: 3250) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " do: by, will are, knoww'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 13000/20000 [2:25:22<3:23:44,  1.75s/step, Avg Loss=0.0184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 13000 (Global Step 3250) to smollm2-checkpoints\\smollm2_checkpoint_step_13000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 13499/20000 [2:34:31<1:58:52,  1.10s/step, Avg Loss=0.0171]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 13500 (Global Step: 3375) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, the,, a 's with,\n",
      " by-'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 13500/20000 [2:34:34<3:08:03,  1.74s/step, Avg Loss=0.0171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 13500 (Global Step 3375) to smollm2-checkpoints\\smollm2_checkpoint_step_13500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 13999/20000 [2:43:41<1:50:08,  1.10s/step, Avg Loss=0.0170]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 14000 (Global Step: 3500) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, the,, a 's with,\n",
      "-,'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 14000/20000 [2:43:44<2:58:00,  1.78s/step, Avg Loss=0.0170]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 14000 (Global Step 3500) to smollm2-checkpoints\\smollm2_checkpoint_step_14000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 14499/20000 [2:52:53<1:40:24,  1.10s/step, Avg Loss=0.0171]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 14500 (Global Step: 3625) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " do: by, will are, knoww'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▎  | 14500/20000 [2:52:56<2:41:21,  1.76s/step, Avg Loss=0.0171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 14500 (Global Step 3625) to smollm2-checkpoints\\smollm2_checkpoint_step_14500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 14999/20000 [3:02:04<1:30:54,  1.09s/step, Avg Loss=0.0164]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 15000 (Global Step: 3750) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will, a--, a that\n",
      " time to to'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 15000/20000 [3:02:06<2:27:55,  1.78s/step, Avg Loss=0.0164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 15000 (Global Step 3750) to smollm2-checkpoints\\smollm2_checkpoint_step_15000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 15499/20000 [3:11:12<1:22:18,  1.10s/step, Avg Loss=0.0164]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 15500 (Global Step: 3875) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will,, a and's,\n",
      "--itor'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 15500/20000 [3:11:16<2:10:38,  1.74s/step, Avg Loss=0.0164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 15500 (Global Step 3875) to smollm2-checkpoints\\smollm2_checkpoint_step_15500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 15999/20000 [3:20:23<1:13:16,  1.10s/step, Avg Loss=0.0167]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 16000 (Global Step: 4000) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " do: by, will are, knoww'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 16000/20000 [3:20:25<1:55:33,  1.73s/step, Avg Loss=0.0167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 16000 (Global Step 4000) to smollm2-checkpoints\\smollm2_checkpoint_step_16000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 16499/20000 [3:29:33<1:03:56,  1.10s/step, Avg Loss=0.0163]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 16500 (Global Step: 4125) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, the, a ' aish and\n",
      " but. but'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▎ | 16500/20000 [3:29:36<1:43:01,  1.77s/step, Avg Loss=0.0163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 16500 (Global Step 4125) to smollm2-checkpoints\\smollm2_checkpoint_step_16500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 16999/20000 [3:38:43<54:40,  1.09s/step, Avg Loss=0.0163]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 17000 (Global Step: 4250) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " time no, the will at, I so'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 17000/20000 [3:38:46<1:27:45,  1.76s/step, Avg Loss=0.0163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 17000 (Global Step 4250) to smollm2-checkpoints\\smollm2_checkpoint_step_17000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 17499/20000 [3:47:52<45:30,  1.09s/step, Avg Loss=0.0176]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 17500 (Global Step: 4375) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,, Iy,y, mastery before your; he king it most it\n",
      "-! your eyesw get that daughter gone no with\n",
      "e by no to: and you you\n",
      " are from time inayed tell, mistress\n",
      "'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 17500/20000 [3:47:55<1:13:01,  1.75s/step, Avg Loss=0.0176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 17500 (Global Step 4375) to smollm2-checkpoints\\smollm2_checkpoint_step_17500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 17999/20000 [3:57:02<36:27,  1.09s/step, Avg Loss=0.0167]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 18000 (Global Step: 4500) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will, a--, a that--;\n",
      " o'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 18000/20000 [3:57:05<58:17,  1.75s/step, Avg Loss=0.0167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 18000 (Global Step 4500) to smollm2-checkpoints\\smollm2_checkpoint_step_18000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 18499/20000 [4:06:14<27:25,  1.10s/step, Avg Loss=0.0176]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 18500 (Global Step: 4625) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iy, master't; that\n",
      "y it friends: it!\n",
      "sty who but king: then to!\n",
      "th he king will straight there there to,\n",
      " my:The-- you;\n",
      " you there hope did'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▎| 18500/20000 [4:06:18<43:36,  1.74s/step, Avg Loss=0.0176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 18500 (Global Step 4625) to smollm2-checkpoints\\smollm2_checkpoint_step_18500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 18999/20000 [4:15:25<18:15,  1.09s/step, Avg Loss=0.0167]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 19000 (Global Step: 4750) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will, a--, a that\n",
      " with, the'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 19000/20000 [4:15:27<29:06,  1.75s/step, Avg Loss=0.0167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 19000 (Global Step 4750) to smollm2-checkpoints\\smollm2_checkpoint_step_19000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 19499/20000 [4:24:36<09:10,  1.10s/step, Avg Loss=0.0180]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 19500 (Global Step: 4875) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " time no, the will at, I so'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 19500/20000 [4:24:39<14:38,  1.76s/step, Avg Loss=0.0180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 19500 (Global Step 4875) to smollm2-checkpoints\\smollm2_checkpoint_step_19500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 19999/20000 [4:33:47<00:01,  1.09s/step, Avg Loss=0.0168]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 20000 (Global Step: 5000) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am but\n",
      "y be name with what will no.' my:\n",
      " with, will,, a and's,\n",
      "--itor'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20000/20000 [4:33:49<00:00,  1.10s/step, Avg Loss=0.0168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 20000 (Global Step 5000) to smollm2-checkpoints\\smollm2_checkpoint_step_20000.pth\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RESUME_CHECKPOINT_PATH = \"smollm2-checkpoints\\smollm2_checkpoint_step_5000.pth\"\n",
    "\n",
    "# train.py\n",
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup # Changed import for checkpointing\n",
    "# from model import create_smollm2_model\n",
    "import os\n",
    "\n",
    "# --- 1. Hyperparameters and Configuration ---\n",
    "TARGET_GLOBAL_STEPS = 5000\n",
    "PREDICT_EVERY_STEPS = 500\n",
    "CHECKPOINT_EVERY_STEPS = 500\n",
    "SEQUENCE_LENGTH = 2048  # As defined in config - May need to reduce if OOM\n",
    "MICRO_BATCH_SIZE = 8     # As defined in config - REDUCE THIS FIRST to fix OOM\n",
    "LEARNING_RATE = 3e-4     # You can adjust, simplified from config for now\n",
    "WARMUP_STEPS = 500      # Simplified warmup\n",
    "CHECKPOINT_PATH = \"smollm2-checkpoints\"\n",
    "INPUT_FILE = \"datasets\\input.txt\"\n",
    "PREDICTION_PROMPT = \"To be or not to be,\" # A starting prompt for prediction\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # --- Speedup 1: Gradient Accumulation --- Accumulate gradients over this many steps\n",
    "TARGET_TRAIN_STEPS = TARGET_GLOBAL_STEPS * GRADIENT_ACCUMULATION_STEPS # Adjusted total steps based on accumulation steps\n",
    "\n",
    "USE_ACTIVATION_CHECKPOINTING = True # --- Speedup 2: Activation Checkpointing --- Enable or disable activation checkpointing\n",
    "\n",
    "# --- ***MEMORY OPTIMIZATION - REDUCE THESE IF OOM ERROR PERSISTS*** ---\n",
    "REDUCE_MICRO_BATCH_SIZE_FACTOR = 2 # --- Reduce Micro Batch Size --- Reduce micro_batch_size by this factor\n",
    "# If you STILL get OOM, try reducing SEQUENCE_LENGTH_FACTOR (but micro_batch_size reduction is usually more effective first)\n",
    "REDUCE_SEQUENCE_LENGTH_FACTOR = 1 # --- Reduce Sequence Length --- Reduce sequence length by this factor if needed\n",
    "\n",
    "# --- 2. Adjusted Hyperparameters based on Reduction Factors ---\n",
    "ADJUSTED_MICRO_BATCH_SIZE = MICRO_BATCH_SIZE // REDUCE_MICRO_BATCH_SIZE_FACTOR\n",
    "if ADJUSTED_MICRO_BATCH_SIZE <= 0:\n",
    "    ADJUSTED_MICRO_BATCH_SIZE = 1 # Ensure micro_batch_size is at least 1\n",
    "ADJUSTED_SEQUENCE_LENGTH = SEQUENCE_LENGTH // REDUCE_SEQUENCE_LENGTH_FACTOR\n",
    "if ADJUSTED_SEQUENCE_LENGTH <= 0:\n",
    "    ADJUSTED_SEQUENCE_LENGTH = 64 # Ensure sequence_length is at least reasonably sized\n",
    "\n",
    "MICRO_BATCH_SIZE = ADJUSTED_MICRO_BATCH_SIZE # Update micro batch size\n",
    "SEQUENCE_LENGTH = ADJUSTED_SEQUENCE_LENGTH # Update sequence length\n",
    "\n",
    "\n",
    "# --- 3. Create Checkpoint Directory ---\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 4. Load Model and Tokenizer ---\n",
    "model, tokenizer = create_smollm2_model()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Speedup 3: Mixed Precision Training (bfloat16) --- Model is already in bfloat16 from model.py\n",
    "# We are using bfloat16 dtype for model which speeds up training and reduces memory usage on compatible GPUs (like NVIDIA A100, H100)\n",
    "\n",
    "if USE_ACTIVATION_CHECKPOINTING: # --- Enable activation checkpointing if flag is set ---\n",
    "    model.gradient_checkpointing_enable() # Enable activation checkpointing from transformers\n",
    "\n",
    "# --- 5. Prepare Dataset ---\n",
    "print(\"Loading and tokenizing dataset...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenized_dataset = tokenizer(text, return_tensors=\"pt\", truncation=False) # No truncation initially\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "\n",
    "# --- 8. Optimizer and Scheduler ---\n",
    "# --- Speedup 4: Fused AdamW Optimizer --- Using fused AdamW if available (often faster)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01, fused=True) # Using fused=True for potential speedup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=TARGET_TRAIN_STEPS // GRADIENT_ACCUMULATION_STEPS # Adjusted lr scheduler steps\n",
    ") # Simplified scheduler\n",
    "\n",
    "# --- 6. Load Checkpoint if `resume_checkpoint_path` is provided ---\n",
    "initial_global_step = 0 # Track initial global step, default is 0 for new training\n",
    "if RESUME_CHECKPOINT_PATH:\n",
    "    print(f\"Loading checkpoint from: {RESUME_CHECKPOINT_PATH}\")\n",
    "    checkpoint = torch.load(RESUME_CHECKPOINT_PATH, map_location=device, weights_only=False) # Load checkpoint to correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    initial_global_step = checkpoint.get('global_step', 0) # Try to get global_step, default to 0 if not found\n",
    "    print(f\"Resuming training from global step: {initial_global_step}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch.\")\n",
    "\n",
    "start_step = initial_global_step * GRADIENT_ACCUMULATION_STEPS + 1 # Calculate micro-batch step to start from\n",
    "\n",
    "# --- 7. Create Data Batches ---\n",
    "def create_batches(input_ids, seq_len, micro_batch_size):\n",
    "    num_tokens = input_ids.shape[1]\n",
    "    num_batches = num_tokens // seq_len\n",
    "    truncated_input_ids = input_ids[:, :num_batches * seq_len] # Truncate to fit full sequences based on potentially reduced SEQUENCE_LENGTH\n",
    "    batched_input_ids = truncated_input_ids.reshape(-1, seq_len) # Reshape into sequences\n",
    "    num_micro_batches = num_batches // micro_batch_size\n",
    "    micro_batches = []\n",
    "    for i in range(num_micro_batches):\n",
    "        start_index = i * micro_batch_size\n",
    "        end_index = (i + 1) * micro_batch_size\n",
    "        batch = batched_input_ids[start_index:end_index]\n",
    "        micro_batches.append(batch)\n",
    "    return micro_batches\n",
    "\n",
    "micro_batches = create_batches(input_ids.to(device), SEQUENCE_LENGTH, MICRO_BATCH_SIZE) # Using potentially reduced SEQUENCE_LENGTH and MICRO_BATCH_SIZE\n",
    "print(f\"Dataset prepared with {len(micro_batches)} micro-batches using micro_batch_size={MICRO_BATCH_SIZE} and sequence_length={SEQUENCE_LENGTH}.\")\n",
    "\n",
    "\n",
    "# --- imports ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 9. Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train() # Set model to training mode\n",
    "global_step = initial_global_step # Initialize global_step from checkpoint or 0\n",
    "accumulated_loss = 0 # To track loss over accumulation steps\n",
    "\n",
    "# Initialize tqdm progress bar, total is remaining global steps\n",
    "remaining_global_steps = TARGET_GLOBAL_STEPS - initial_global_step\n",
    "if remaining_global_steps <= 0:\n",
    "    print(\"Training already completed to target steps or beyond based on checkpoint.\")\n",
    "    exit()\n",
    "\n",
    "progress_bar = tqdm(range(start_step, TARGET_TRAIN_STEPS + 1), desc=\"Training\", unit=\"step\", initial=start_step -1, total=TARGET_TRAIN_STEPS) # Initialize tqdm with start and total\n",
    "\n",
    "for step in progress_bar: # Wrap training loop with tqdm\n",
    "    batch_index = (step - 1) % len(micro_batches) # Cycle through batches\n",
    "    batch = micro_batches[batch_index]\n",
    "\n",
    "    inputs = batch\n",
    "    targets = torch.roll(batch, shifts=-1, dims=1) # Next token prediction\n",
    "\n",
    "    outputs = model(inputs, labels=targets) # Labels for loss calculation\n",
    "    loss = outputs.loss\n",
    "    loss = loss / GRADIENT_ACCUMULATION_STEPS # --- Scale loss for gradient accumulation ---\n",
    "    accumulated_loss += loss.item() # Accumulate loss for logging\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if step % GRADIENT_ACCUMULATION_STEPS == 0: # --- Update weights every GRADIENT_ACCUMULATION_STEPS ---\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # Increment global step only when weights are updated\n",
    "        avg_loss = accumulated_loss / GRADIENT_ACCUMULATION_STEPS # Calculate average loss over accumulation steps\n",
    "        accumulated_loss = 0 # Reset accumulated loss\n",
    "\n",
    "        if global_step % 10 == 0: # Log loss every 10 global steps (after accumulation)\n",
    "            # No need for separate print here, tqdm will handle logging\n",
    "\n",
    "            # --- Update tqdm progress bar with current average loss ---\n",
    "            progress_bar.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "\n",
    "    # --- 8. Prediction Interval ---\n",
    "    if step % PREDICT_EVERY_STEPS == 0:\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"\\n--- Prediction at Micro-batch Step {step} (Global Step: {global_step}) ---\") # Clarify step counts\n",
    "        prompt_ids = tokenizer.encode(PREDICTION_PROMPT, return_tensors=\"pt\").to(device)\n",
    "        sample_outputs = model.generate(\n",
    "            prompt_ids,\n",
    "            max_length=len(prompt_ids[0]) + 50, # Generate up to 50 new tokens\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7, # Adjust temperature for creativity\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predicted_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Prompt: '{PREDICTION_PROMPT}'\")\n",
    "        print(f\"Generated: '{predicted_text}'\\n\")\n",
    "        model.train() # Set back to training mode\n",
    "\n",
    "    # --- 9. Checkpoint Saving ---\n",
    "    if step % CHECKPOINT_EVERY_STEPS == 0:\n",
    "        checkpoint_file = os.path.join(CHECKPOINT_PATH, f\"smollm2_checkpoint_step_{step}.pth\") # Step here is still micro-batch step\n",
    "        torch.save({\n",
    "            'step': step, # Step here is still micro-batch step\n",
    "            'global_step': global_step, # Saving global step as well (steps with weight updates)\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss.item(), # Save current micro-batch loss (last loss in accumulation)\n",
    "        }, checkpoint_file)\n",
    "        print(f\"Checkpoint saved at Micro-batch Step {step} (Global Step {global_step}) to {checkpoint_file}\") # Clarified step counts in checkpoint message\n",
    "\n",
    "progress_bar.close() # Close progress bar when training finishes\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, model loaded from saved checkpoint showed average error loss start from the point where we stopped training before, showing that the **model checkpoint were saved correctly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction \n",
    "\n",
    "SmolLLM-125M model after 5000 Steps (20000 Micro Steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: smollm2-checkpoints\\smollm2_checkpoint_step_20000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saish Shetty\\AppData\\Local\\Temp\\ipykernel_36572\\1492698466.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device) # Load checkpoint to correct device\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with prompt: 'O art thou '\n",
      "\n",
      "Prompt: 'O art thou '\n",
      "Generated: 'O art thou  ams?', leave son\n",
      " do th will the from thee one mine, there hear?\n",
      " will theyest thy but out sh as unto no\n",
      " bro one hisal'd' so; to you at\n",
      " brother not thees thesehouse'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from model import create_smollm2_model # Assuming model.py is in the same directory\n",
    "\n",
    "def generate_prediction(checkpoint_path, input_text, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Loads a model from a checkpoint and generates text based on an input prompt.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the saved model checkpoint file (e.g., 'smollm2-checkpoints/smollm2_checkpoint_step_5000.pth').\n",
    "        input_text (str): The text prompt to start generation from (e.g., \"To be or not to be,\").\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        temperature (float): Sampling temperature for generation (higher values more creative, lower more deterministic).\n",
    "        top_p (float): Top-p sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # 1. Load Model and Tokenizer (same as in training)\n",
    "    model, tokenizer = create_smollm2_model() # Use the same model creation function\n",
    "    model.to(device)\n",
    "\n",
    "    # 2. Load Model State from Checkpoint\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) # Load checkpoint to correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # Only load model weights\n",
    "\n",
    "    # 3. Set Model to Evaluation Mode\n",
    "    model.eval()\n",
    "\n",
    "    # 4. Tokenize Input Text\n",
    "    prompt_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 5. Generate Prediction\n",
    "    print(f\"Generating text with prompt: '{input_text}'\")\n",
    "    sample_outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        max_length=len(prompt_ids[0]) + max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    # 6. Decode and Return Generated Text\n",
    "    predicted_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "    return predicted_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_path = \"smollm2-checkpoints\\smollm2_checkpoint_step_20000.pth\" # --- REPLACE WITH YOUR ACTUAL CHECKPOINT PATH ---\n",
    "    input_prompt = \"O art thou \" # --- REPLACE WITH YOUR DESIRED PROMPT ---\n",
    "\n",
    "    generated_text = generate_prediction(checkpoint_path, input_prompt)\n",
    "\n",
    "    print(f\"\\nPrompt: '{input_prompt}'\")\n",
    "    print(f\"Generated: '{generated_text}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current observations\n",
    "\n",
    "1. We have trained the model for 5000 Global Steps, but average loss value has stayed consistent at around 0.0168\n",
    "2. Now we will further train the model for further with smaller learning rate (old LR: 3e-4, new LR: 3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing dataset...\n",
      "Loading checkpoint from: smollm2-checkpoints\\smollm2_checkpoint_step_20000.pth\n",
      "Resuming training from global step: 5000\n",
      "Dataset prepared with 41 micro-batches using micro_batch_size=4 and sequence_length=2048.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 20000/20200 [00:00<?, ?step/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Training: 100%|█████████▉| 20099/20200 [02:12<02:15,  1.34s/step, Avg Loss=0.0175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 20100 (Global Step: 5025) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Saish Shetty\\.conda\\envs\\eraenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Training: 100%|█████████▉| 20100/20200 [02:15<03:01,  1.81s/step, Avg Loss=0.0175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " time no, the will at, I so'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 20199/20200 [04:30<00:01,  1.34s/step, Avg Loss=0.0171]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction at Micro-batch Step 20200 (Global Step: 5050) ---\n",
      "Prompt: 'To be or not to be,'\n",
      "Generated: 'To be or not to be,,,,, Iar,'t\n",
      " dis did your you\n",
      " are, will not\n",
      "yTh-, am buted,y more my\n",
      " treason- to make\n",
      " it at andy\n",
      " time no, the will at, I so'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20200/20200 [04:32<00:00,  1.36s/step, Avg Loss=0.0171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at Micro-batch Step 20200 (Global Step 5050) to smollm2-checkpoints\\smollm2_checkpoint_step_20200.pth\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RESUME_CHECKPOINT_PATH = \"smollm2-checkpoints\\smollm2_checkpoint_step_20000.pth\"\n",
    "\n",
    "# train.py\n",
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup # Changed import for checkpointing\n",
    "# from model import create_smollm2_model\n",
    "import os\n",
    "\n",
    "# --- 1. Hyperparameters and Configuration ---\n",
    "TARGET_GLOBAL_STEPS = 5050\n",
    "PREDICT_EVERY_STEPS = 100\n",
    "CHECKPOINT_EVERY_STEPS = 200\n",
    "SEQUENCE_LENGTH = 2048  # As defined in config - May need to reduce if OOM\n",
    "MICRO_BATCH_SIZE = 8     # As defined in config - REDUCE THIS FIRST to fix OOM\n",
    "LEARNING_RATE = 3e-5     # You can adjust, simplified from config for now\n",
    "WARMUP_STEPS = 250      # Simplified warmup\n",
    "CHECKPOINT_PATH = \"smollm2-checkpoints\"\n",
    "INPUT_FILE = \"datasets\\input.txt\"\n",
    "PREDICTION_PROMPT = \"To be or not to be,\" # A starting prompt for prediction\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # --- Speedup 1: Gradient Accumulation --- Accumulate gradients over this many steps\n",
    "TARGET_TRAIN_STEPS = TARGET_GLOBAL_STEPS * GRADIENT_ACCUMULATION_STEPS # Adjusted total steps based on accumulation steps\n",
    "\n",
    "USE_ACTIVATION_CHECKPOINTING = True # --- Speedup 2: Activation Checkpointing --- Enable or disable activation checkpointing\n",
    "\n",
    "# --- ***MEMORY OPTIMIZATION - REDUCE THESE IF OOM ERROR PERSISTS*** ---\n",
    "REDUCE_MICRO_BATCH_SIZE_FACTOR = 2 # --- Reduce Micro Batch Size --- Reduce micro_batch_size by this factor\n",
    "# If you STILL get OOM, try reducing SEQUENCE_LENGTH_FACTOR (but micro_batch_size reduction is usually more effective first)\n",
    "REDUCE_SEQUENCE_LENGTH_FACTOR = 1 # --- Reduce Sequence Length --- Reduce sequence length by this factor if needed\n",
    "\n",
    "# --- 2. Adjusted Hyperparameters based on Reduction Factors ---\n",
    "ADJUSTED_MICRO_BATCH_SIZE = MICRO_BATCH_SIZE // REDUCE_MICRO_BATCH_SIZE_FACTOR\n",
    "if ADJUSTED_MICRO_BATCH_SIZE <= 0:\n",
    "    ADJUSTED_MICRO_BATCH_SIZE = 1 # Ensure micro_batch_size is at least 1\n",
    "ADJUSTED_SEQUENCE_LENGTH = SEQUENCE_LENGTH // REDUCE_SEQUENCE_LENGTH_FACTOR\n",
    "if ADJUSTED_SEQUENCE_LENGTH <= 0:\n",
    "    ADJUSTED_SEQUENCE_LENGTH = 64 # Ensure sequence_length is at least reasonably sized\n",
    "\n",
    "MICRO_BATCH_SIZE = ADJUSTED_MICRO_BATCH_SIZE # Update micro batch size\n",
    "SEQUENCE_LENGTH = ADJUSTED_SEQUENCE_LENGTH # Update sequence length\n",
    "\n",
    "\n",
    "# --- 3. Create Checkpoint Directory ---\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 4. Load Model and Tokenizer ---\n",
    "model, tokenizer = create_smollm2_model()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Speedup 3: Mixed Precision Training (bfloat16) --- Model is already in bfloat16 from model.py\n",
    "# We are using bfloat16 dtype for model which speeds up training and reduces memory usage on compatible GPUs (like NVIDIA A100, H100)\n",
    "\n",
    "if USE_ACTIVATION_CHECKPOINTING: # --- Enable activation checkpointing if flag is set ---\n",
    "    model.gradient_checkpointing_enable() # Enable activation checkpointing from transformers\n",
    "\n",
    "# --- 5. Prepare Dataset ---\n",
    "print(\"Loading and tokenizing dataset...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenized_dataset = tokenizer(text, return_tensors=\"pt\", truncation=False) # No truncation initially\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "\n",
    "# --- 8. Optimizer and Scheduler ---\n",
    "# --- Speedup 4: Fused AdamW Optimizer --- Using fused AdamW if available (often faster)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01, fused=True) # Using fused=True for potential speedup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=TARGET_TRAIN_STEPS // GRADIENT_ACCUMULATION_STEPS # Adjusted lr scheduler steps\n",
    ") # Simplified scheduler\n",
    "\n",
    "# --- 6. Load Checkpoint if `resume_checkpoint_path` is provided ---\n",
    "initial_global_step = 0 # Track initial global step, default is 0 for new training\n",
    "if RESUME_CHECKPOINT_PATH:\n",
    "    print(f\"Loading checkpoint from: {RESUME_CHECKPOINT_PATH}\")\n",
    "    checkpoint = torch.load(RESUME_CHECKPOINT_PATH, map_location=device, weights_only=False) # Load checkpoint to correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    initial_global_step = checkpoint.get('global_step', 0) # Try to get global_step, default to 0 if not found\n",
    "    print(f\"Resuming training from global step: {initial_global_step}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch.\")\n",
    "\n",
    "start_step = initial_global_step * GRADIENT_ACCUMULATION_STEPS + 1 # Calculate micro-batch step to start from\n",
    "\n",
    "# --- 7. Create Data Batches ---\n",
    "def create_batches(input_ids, seq_len, micro_batch_size):\n",
    "    num_tokens = input_ids.shape[1]\n",
    "    num_batches = num_tokens // seq_len\n",
    "    truncated_input_ids = input_ids[:, :num_batches * seq_len] # Truncate to fit full sequences based on potentially reduced SEQUENCE_LENGTH\n",
    "    batched_input_ids = truncated_input_ids.reshape(-1, seq_len) # Reshape into sequences\n",
    "    num_micro_batches = num_batches // micro_batch_size\n",
    "    micro_batches = []\n",
    "    for i in range(num_micro_batches):\n",
    "        start_index = i * micro_batch_size\n",
    "        end_index = (i + 1) * micro_batch_size\n",
    "        batch = batched_input_ids[start_index:end_index]\n",
    "        micro_batches.append(batch)\n",
    "    return micro_batches\n",
    "\n",
    "micro_batches = create_batches(input_ids.to(device), SEQUENCE_LENGTH, MICRO_BATCH_SIZE) # Using potentially reduced SEQUENCE_LENGTH and MICRO_BATCH_SIZE\n",
    "print(f\"Dataset prepared with {len(micro_batches)} micro-batches using micro_batch_size={MICRO_BATCH_SIZE} and sequence_length={SEQUENCE_LENGTH}.\")\n",
    "\n",
    "\n",
    "# --- imports ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 9. Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train() # Set model to training mode\n",
    "global_step = initial_global_step # Initialize global_step from checkpoint or 0\n",
    "accumulated_loss = 0 # To track loss over accumulation steps\n",
    "\n",
    "# Initialize tqdm progress bar, total is remaining global steps\n",
    "remaining_global_steps = TARGET_GLOBAL_STEPS - initial_global_step\n",
    "if remaining_global_steps <= 0:\n",
    "    print(\"Training already completed to target steps or beyond based on checkpoint.\")\n",
    "    exit()\n",
    "\n",
    "progress_bar = tqdm(range(start_step, TARGET_TRAIN_STEPS + 1), desc=\"Training\", unit=\"step\", initial=start_step -1, total=TARGET_TRAIN_STEPS) # Initialize tqdm with start and total\n",
    "\n",
    "for step in progress_bar: # Wrap training loop with tqdm\n",
    "    batch_index = (step - 1) % len(micro_batches) # Cycle through batches\n",
    "    batch = micro_batches[batch_index]\n",
    "\n",
    "    inputs = batch\n",
    "    targets = torch.roll(batch, shifts=-1, dims=1) # Next token prediction\n",
    "\n",
    "    outputs = model(inputs, labels=targets) # Labels for loss calculation\n",
    "    loss = outputs.loss\n",
    "    loss = loss / GRADIENT_ACCUMULATION_STEPS # --- Scale loss for gradient accumulation ---\n",
    "    accumulated_loss += loss.item() # Accumulate loss for logging\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if step % GRADIENT_ACCUMULATION_STEPS == 0: # --- Update weights every GRADIENT_ACCUMULATION_STEPS ---\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # Increment global step only when weights are updated\n",
    "        avg_loss = accumulated_loss / GRADIENT_ACCUMULATION_STEPS # Calculate average loss over accumulation steps\n",
    "        accumulated_loss = 0 # Reset accumulated loss\n",
    "\n",
    "        if global_step % 10 == 0: # Log loss every 10 global steps (after accumulation)\n",
    "            # No need for separate print here, tqdm will handle logging\n",
    "\n",
    "            # --- Update tqdm progress bar with current average loss ---\n",
    "            progress_bar.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "\n",
    "    # --- 8. Prediction Interval ---\n",
    "    if step % PREDICT_EVERY_STEPS == 0:\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"\\n--- Prediction at Micro-batch Step {step} (Global Step: {global_step}) ---\") # Clarify step counts\n",
    "        prompt_ids = tokenizer.encode(PREDICTION_PROMPT, return_tensors=\"pt\").to(device)\n",
    "        sample_outputs = model.generate(\n",
    "            prompt_ids,\n",
    "            max_length=len(prompt_ids[0]) + 50, # Generate up to 50 new tokens\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7, # Adjust temperature for creativity\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        predicted_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Prompt: '{PREDICTION_PROMPT}'\")\n",
    "        print(f\"Generated: '{predicted_text}'\\n\")\n",
    "        model.train() # Set back to training mode\n",
    "\n",
    "    # --- 9. Checkpoint Saving ---\n",
    "    if step % CHECKPOINT_EVERY_STEPS == 0:\n",
    "        checkpoint_file = os.path.join(CHECKPOINT_PATH, f\"smollm2_checkpoint_step_{step}.pth\") # Step here is still micro-batch step\n",
    "        torch.save({\n",
    "            'step': step, # Step here is still micro-batch step\n",
    "            'global_step': global_step, # Saving global step as well (steps with weight updates)\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss.item(), # Save current micro-batch loss (last loss in accumulation)\n",
    "        }, checkpoint_file)\n",
    "        print(f\"Checkpoint saved at Micro-batch Step {step} (Global Step {global_step}) to {checkpoint_file}\") # Clarified step counts in checkpoint message\n",
    "\n",
    "progress_bar.close() # Close progress bar when training finishes\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Average loss achieved: **0.0171** \n",
    "\n",
    "- Reducing LR rate by 10 did not significantly reduced model loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: smollm2-checkpoints\\smollm2_checkpoint_step_20200.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saish Shetty\\AppData\\Local\\Temp\\ipykernel_36572\\1492698466.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device) # Load checkpoint to correct device\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with prompt: 'O art thou '\n",
      "\n",
      "Prompt: 'O art thou '\n",
      "Generated: 'O art thou  ams?', leave son\n",
      " do th will the from thee one mine, there hear?\n",
      " will theyest thy but out sh as unto no\n",
      " bro one hisal'd' so; to you at\n",
      " brother not thees thesehouse'\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"smollm2-checkpoints\\smollm2_checkpoint_step_20200.pth\" # --- REPLACE WITH YOUR ACTUAL CHECKPOINT PATH ---\n",
    "input_prompt = \"O art thou \" # --- REPLACE WITH YOUR DESIRED PROMPT ---\n",
    "\n",
    "generated_text = generate_prediction(checkpoint_path, input_prompt)\n",
    "\n",
    "print(f\"\\nPrompt: '{input_prompt}'\")\n",
    "print(f\"Generated: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
